# -*- coding: utf-8 -*-
"""RAG System Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KIAzU8_weHMs-Ba6wAzfQQcmWTGO2kLw

It builds a simple RAG pipeline: read a PDF, split it into chunks, embed with SentenceTransformers, index with FAISS, retrieve relevant chunks, then answer with a small LLaMA model)

Step 1: Install Required Libraries
"""

!pip install -q llama-index PyMuPDF sentence-transformers faiss-cpu transformers

"""Step 2: Upload and Extract PDF Content"""

from google.colab import files #imports Colab’s file-upload helper (so you can pick a file from your computer).
import fitz  # loads PyMuPDF (library to open and read PDFs).

uploaded = files.upload()

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path) #defines a function to read all pages of a PDF and collect their text.
    text = ""  #creates an empty string to accumulate text.
    for page in doc:
        text += page.get_text()
    return text

pdf_filename = next(iter(uploaded))
document_text = extract_text_from_pdf(pdf_filename)
print(document_text[:1000])  # prints the first 1,000 characters so you can sanity-check the extraction.

"""Step 3: Create Embeddings and Store in FAISS Index"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

# Split text into chunks
def split_text(text, max_length=500):
    return [text[i:i+max_length] for i in range(0, len(text), max_length)]

chunks = split_text(document_text)
embeddings = model.encode(chunks)

# FAISS index
dimension = embeddings.shape[1] #Determines the number of features (dimensions) in each embedding vector.
#embeddings is a 2D array where each row is an embedding vector. shape gives the size of each vector (number of columns).

index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

# Store mapping of index to chunk
chunk_mapping = {i: chunk for i, chunk in enumerate(chunks)}

"""SentenceTransformer is the embedding model class (turns text into vectors).

faiss is Facebook AI Similarity Search (fast vector search).

numpy is used for arrays.

model = SentenceTransformer('all-MiniLM-L6-v2') loads a small, fast sentence-embedding model.

split_text(...) slices the big document into 500-character pieces (simple, but works).

chunks = split_text(document_text) creates the list of chunks.

embeddings = model.encode(chunks) turns each chunk into a vector (array of numbers).

dimension = embeddings.shape[1] gets the vector size (columns of the 2D array).

index = faiss.IndexFlatL2(dimension) builds a FAISS index that uses L2 distance.

index.add(...) inserts all the chunk embeddings into the index.

chunk_mapping = {i: chunk ...} remembers which chunk (text) corresponds to each embedding row.

Step 4: Load a LLaMA-based Model (via HuggingFace Transformers)
"""

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')  # Embedding model Re-imports and re-creates the same embedding model as above.
#(Tip: this duplication isn’t harmful, but you don’t need it if Cell 5 already ran.)

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# Use a smaller, Colab-friendly LLaMA model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
llama_model = AutoModelForCausalLM.from_pretrained(model_name)

# Set up generation pipeline (use GPU if available)
generator = pipeline("text-generation", model=llama_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

"""-Imports Hugging Face Transformers utilities and PyTorch.

-model_name picks a small chat model to keep resource usage low.

-AutoTokenizer.from_pretrained(...) downloads the matching tokenizer.

-AutoModelForCausalLM.from_pretrained(...) downloads the language model.

-pipeline("text-generation", ...) wraps the model+tokenizer into a convenient generator.

-device=0 if ... else -1 uses GPU if available, otherwise CPU.

Step 5: Query the Document (RAG Retrieval + Generation)
"""

def retrieve_relevant_chunks(query, top_k=4): #  top_k = 3: specifies how many of the most relevant document chunks should be retrieved
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding), top_k)
    # index is the FAISS index which was created earlier, which contains the embeddings of all the document chunks
    # index.search() is a method that searches this index for embeddings that are similar to the query_embedding
    #The method returns two things:
    #distances: The calculated distances between the query embedding and the top_k most similar chunk embeddings. A smaller distance means higher similarity.
    #indices: The indices (positions) in the original chunks list that correspond to the top_k most similar chunk embeddings.
    return [chunk_mapping[i] for i in indices[0]]

def generate_answer(query):
    context_chunks = retrieve_relevant_chunks(query)
    context = "\n".join(context_chunks)

    prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
    output = generator(prompt, max_new_tokens=200, do_sample=True)[0]['generated_text']
    return output

"""-retrieve_relevant_chunks(query, top_k) embeds the user question and searches the FAISS index.

-index.search(...) finds the top_k most similar chunks.

-return [chunk_mapping[i] ...] converts those indices back into the original text chunks.

-generate_answer(query) pulls those chunks, stitches them into a context string, and builds a prompt:

-The prompt format gives the model the Context and the Question.

-generator(..., max_new_tokens=200, do_sample=True) creates the answer text.

Example Usage
"""

generate_answer("what is GSI?")

generate_answer("How many hotel chains are participating in HRS’ Green Stay Initiative?")

generate_answer("Who is the Contact person?")

def retrieve_relevant_chunks(query, top_k=3):
    # Step 1: Encode the query into an embedding
    query_embedding = model.encode([query])  # Encoding the query

    # Step 2: Perform the similarity search in the FAISS index
    distances, indices = index.search(np.array(query_embedding), top_k)  # `index.search()` returns distances and indices

    # Step 3: Print the distances and indices for debugging purposes
    print(f"Distances: {distances}")  # Show how similar the retrieved chunks are to the query
    print(f"Indices: {indices}")      # Show the positions of the top-k chunks in the original chunk list

    # Step 4: Print the chunk mapping with chunk number (optional for clarity)
    print("\nChunk Mapping (Top-k Chunks):")
    for i, idx in enumerate(indices[0]):
        print(f"Chunk {i + 1}: {chunk_mapping[idx]} (Index: {idx}) - Distance: {distances[0][i]}")

    # Step 5: Return the relevant chunks from the chunk mapping
    return [chunk_mapping[i] for i in indices[0]]

!pip install jedi